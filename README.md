# monocular-depth-estimation
## 2014
* Deep Convolutional Neural Fields for Depth Estimation from a Single Image 
  + [paper](https://arxiv.org/abs/1411.6387)
* Depth Map Prediction from a Single Image using a Multi-Scale Deep Network
  + [paper](https://arxiv.org/abs/1406.2283v1)
  + [code for pytorch](https://github.com/DhruvJawalkar/Depth-Map-Prediction-from-a-Single-Image-using-a-Multi-Scale-Deep-Network)
## 2015
* Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture
  + [paper](https://arxiv.org/abs/1411.4734)
## 2016
* Joint Semantic Segmentation and Depth Estimation with Deep Convolutional Networks
  + [paper](https://arxiv.org/abs/1604.07480v1)
* Deeper Depth Prediction with Fully Convolutional Residual Networks
   + [paper](https://arxiv.org/abs/1606.00373)
* Deep3D Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks
  + [paper](https://arxiv.org/abs/1604.03650v1)
## 2017
* A Two-Streamed Network for Estimating Fine-Scaled Depth Maps from Single RGB Images
  + [paper](https://arxiv.org/abs/1607.00730)
* Single-Image Depth Perception in the Wild
  + [paper](https://arxiv.org/abs/1604.03901)
* Unsupervised Learning of Depth and Ego-Motion from Video
  + [paper](https://github.com/tinghuiz/SfMLearner)
  + [code](https://github.com/tinghuiz/SfMLearner)
  + [project webpage](https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/)(已复现)
  
 
 
## 2018
* Epipolar Geometry based Learning of Multi-view Depth and Ego-Motion from Monocular Sequences
  + [paper](https://arxiv.org/abs/1812.11922)
* SfMLearner++: Learning Monocular Depth & Ego-Motion using Meaningful Geometric Constraints
  + [paper](https://arxiv.org/abs/1812.08370)
* Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion
  + [paper](https://arxiv.org/abs/1812.08156)
* Fast and Accurate Depth Estimation from Sparse Light Fields
  + [paper](https://arxiv.org/abs/1812.06856)
  
  
  
  

## 2019
* DFineNet: Ego-Motion Estimation and Depth Refinement from Sparse, Noisy Depth Input with RGB Guidance
  + [paper](https://arxiv.org/abs/1903.06397)
* Bilateral Cyclic Constraint and Adaptive Regularization for Unsupervised Monocular Depth Prediction
  + [paper](https://arxiv.org/abs/1903.07309)
* Anytime Stereo Image Depth Estimation on Mobile Devices(**)
  + [paper](https://arxiv.org/abs/1810.11408)
* Refine and Distill: Exploiting Cycle-Inconsistency and Knowledge Distillation for Unsupervised Monocular Depth Estimation(CVPR2019)
  + [paper](https://arxiv.org/abs/1903.04202)
* Self-supervised Learning for Single View Depth and Surface Normal Estimation
  + [paper](https://arxiv.org/abs/1903.00112)
* DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image
  + [paper](https://arxiv.org/abs/1812.00488v1)
* A Motion Free Approach to Dense Depth Estimation in Complex Dynamic Scene
  + [paper](https://arxiv.org/abs/1902.03791)
* Self-supervised Learning for Dense Depth Estimation in Monocular Endoscopy
  + [paper](https://arxiv.org/abs/1902.07766?context=cs)
* Region Deformer Networks for Unsupervised Depth Estimation from Unconstrained Monocular Videos
  + [paper](https://arxiv.org/abs/1902.09907)
* Single Image Deblurring and Camera Motion Estimation with Depth Map
  + [paper](https://arxiv.org/abs/1903.00231)
* SweepNet: Wide-baseline Omnidirectional Depth Estimation
  + [paper](https://arxiv.org/abs/1902.10904)
* Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference(重点)
  + [paper](https://arxiv.org/abs/1902.10556)
* Multi-layer Depth and Epipolar Feature Transformers for 3D Scene Reconstruction
  + [paper](https://arxiv.org/abs/1902.06729)
* Depth-Map Generation using Pixel Matching in Stereoscopic Pair of Images
  + [paper](https://arxiv.org/abs/1902.03471)
* Unstructured Multi-View Depth Estimation Using Mask-Based Multiplane Representation
  + [paper](https://arxiv.org/abs/1902.02166)
* DFuseNet: Deep Fusion of RGB and Sparse Depth Information for Image Guided Dense Depth Completion
  + [paper](https://arxiv.org/abs/1902.00761)
* Attention-based Context Aggregation Network for Monocular Depth Estimation
  + [paper](https://arxiv.org/abs/1901.10137)


  


## review
* Monocular Depth Estimation: A Survey
  + [paper](https://arxiv.org/abs/1901.09402)

## Network Architecture
* ResNet: Deep Residual Learning for Image Recognition
  + [paper](https://arxiv.org/abs/1512.03385)
* VGG: Very Deep Convolutional Networks for Large-scale Image Recognition
  + [paper](https://arxiv.org/abs/1409.1556)
* Squeeze-and-Excitation Networks
  + [paper]( https://arxiv.org/abs/1709.01507v1)
* ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices
  + [paper]( https://arxiv.org/abs/1707.01083)
* ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design
  + [paper](https://arxiv.org/abs/1807.11164)
* MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
  + [paper](https://arxiv.org/abs/1704.04861)
* MobileNetV2: Inverted Residuals and Linear Bottlenecks
  + [paper](https://arxiv.org/abs/1801.04381)
  
  
 
  
 

