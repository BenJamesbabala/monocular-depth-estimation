# monocular-depth-estimation

## 2019


* Deep Robust Single Image Depth Estimation Neural Network Using Scene Understanding
  + [paper](https://arxiv.org/abs/1906.03279)
  
* Unsupervised Monocular Depth and Ego-motion Learning with Structure and Semantics
  + [paper](https://arxiv.org/abs/1906.05717)
  + [code](https://sites.google.com/corp/view/struct2depth)
* Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving
  + [paper](https://arxiv.org/abs/1906.06310)
  + [code](https://github.com/mileyan/Pseudo_Lidar_V2)
* Monocular Depth Estimation Using Relative Depth Maps
  + [paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Lee_Monocular_Depth_Estimation_Using_Relative_Depth_Maps_CVPR_2019_paper.pdf)
* Connecting the Dots: Learning Representations for Active Monocular Depth Estimation
  + [paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Riegler_Connecting_the_Dots_Learning_Representations_for_Active_Monocular_Depth_Estimation_CVPR_2019_paper.pdf)
  
* Soft Labels for Ordinal Regression
  + [paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Diaz_Soft_Labels_for_Ordinal_Regression_CVPR_2019_paper.pdf)

* A General and Adaptive Robust Loss Function
  + [paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Barron_A_General_and_Adaptive_Robust_Loss_Function_CVPR_2019_paper.pdf)

* Adversarial Structure Matching for Structured Prediction Tasks
  + [paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Hwang_Adversarial_Structure_Matching_for_Structured_Prediction_Tasks_CVPR_2019_paper.pdf)
* Veritatem Dies Aperit - Temporally Consistent Depth Prediction Enabled by a Multi-Task Geometric and Semantic Scene Understanding Approach
  + [paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Atapour-Abarghouei_Veritatem_Dies_Aperit_-_Temporally_Consistent_Depth_Prediction_Enabled_by_CVPR_2019_paper.pdf)
* Towards Scene Understanding: Unsupervised Monocular Depth Estimation With Semantic-Aware Representation
  + [paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Towards_Scene_Understanding_Unsupervised_Monocular_Depth_Estimation_With_Semantic-Aware_Representation_CVPR_2019_paper.pdf)
* Pattern-Affinitive Propagation across Depth, Surface Normal and Semantic Segmentation
  + [paper](https://arxiv.org/pdf/1906.03525v1.pdf)
* Generating and Exploiting Probabilistic Monocular Depth Estimates
  + [paper](https://arxiv.org/pdf/1906.05739v1.pdf)
* Attention-based Context Aggregation Network for Monocular Depth Estimation
  + [paper](https://arxiv.org/pdf/1901.10137v1.pdf)
  + [code](https://github.com/miraiaroha/ACAN)

* Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud
  + [paper](https://arxiv.org/pdf/1903.09847v3.pdf)
  + [code](https://github.com/xinshuoweng/mono3D_PLiDAR)


* Geometry-Aware Symmetric Domain Adaptation for Monocular Depth Estimation
  + [paper](https://arxiv.org/pdf/1904.01870v1.pdf)
  + [code](https://github.com/sshan-zhao/GASDA)

* Learning monocular depth estimation infusing traditional stereo knowledge
  + [paper](https://arxiv.org/pdf/1904.04144v1.pdf)
  + [code](https://github.com/fabiotosi92/monoResMatch-Tensorflow)
  
* Learn Stereo, Infer Mono: Siamese Networks for Self-Supervised, Monocular, Depth Estimation
  + [paper](https://arxiv.org/pdf/1905.00401v1.pdf)
  + [code](https://github.com/mtngld/lsim)

* PackNet-SfM: 3D Packing for Self-Supervised Monocular Depth Estimation
  + [paper](https://arxiv.org/pdf/1905.02693v1.pdf)
  + [code](https://github.com/ToyotaResearchInstitute/packnet-sfm)
* Semi-Supervised Monocular Depth Estimation with Left-Right Consistency Using Deep Neural Network
  + [paper](https://arxiv.org/pdf/1905.07542v1.pdf)
  + [code](https://github.com/a-jahani/semiDepth)
  
* SharpNet: Fast and Accurate Recovery of Occluding Contours in Monocular Depth Estimation
  + [paper](https://arxiv.org/pdf/1905.08598v1.pdf)
  + [code](https://github.com/MichaelRamamonjisoa/SharpNet)
* Learning the Depths of Moving People by Watching Frozen People
  + [paper](https://arxiv.org/abs/1904.11111)
* Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras
  + [paper](https://arxiv.org/abs/1904.04998)
![结构图](https://raw.githubusercontent.com/CVLAB-Unibo/Learning2AdaptForStereo/master/architecture.png)
* Learning to Adapt for Stereo
  + [paper](https://arxiv.org/abs/1904.02957)
  + [code](https://github.com/CVLAB-Unibo/Learning2AdaptForStereo)
* Student Becoming the Master: Knowledge Amalgamation for Joint Scene Parsing, Depth Estimation, and More
  + [paper](https://arxiv.org/pdf/1904.10167.pdf)
* Learning the Depths of Moving People by Watching Frozen People
  + [paper](https://arxiv.org/pdf/1904.11111.pdf)
* Web Stereo Video Supervision for Depth Prediction from Dynamic Scenes
  + [paper](https://arxiv.org/pdf/1904.11112.pdf) 
  ![结构图](https://raw.githubusercontent.com/dwofk/fast-depth/master/img/visualization.png)
* FastDepth:  Fast  Monocular  Depth  Estimation  on  Embedded  Systems
  + [paper](https://arxiv.org/pdf/1903.03273.pdf)
  + [code](https://github.com/dwofk/fast-depth)
  + [webage](http://fastdepth.mit.edu/)
* AMNet:Deep Atrous Multiscale Stereo Disparity Estimation Networks
  + [paper](https://arxiv.org/abs/1904.09099)
* Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras
  + [paper](https://arxiv.org/)
* Group-wise Correlation Stereo Network
  + [paper](https://arxiv.org/abs/1903.04025)
  + [code](https://github.com/xy-guo/GwcNet)
![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/Refine%20and%20Distill.png)
* Refine and Distill: Exploiting Cycle-Inconsistency and Knowledge Distillation for Unsupervised Monocular Depth Estimation
  + [paper](https://arxiv.org/abs/1903.04202)
  
![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/Two-branch%20%20decoder.png)
* Bilateral Cyclic Constraint and Adaptive Regularization for Unsupervised Monocular Depth Prediction
  + [paper](https://arxiv.org/abs/1903.07309)
![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/DKN.png)
* Deformable kernel networks for guided depth map upsampling
  + [paper](https://arxiv.org/abs/1903.11286?context=cs.CV)
  + [code](https://cvlab-yonsei.github.io/projects/DKN)
![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/multitask%20refinenet.PNG "multitask%20refinenet")
* Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations
  + [paper](https://arxiv.org/pdf/1809.04766.pdf)
  + [code](https://github.com/drsleep/multi-task-refinenet)
![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/Competitive%20Collaboration.PNG "Competitive%20Collaboration")
* Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera
Motion, Optical Flow and Motion Segmentation
  + [paper](https://arxiv.org/pdf/1805.09806.pdf)
  + [code](https://github.com/anuragranj/cc)
  
* Privacy Protection in Street-View Panoramas using Depth and Multi-View Imagery
  + [paper](https://arxiv.org/abs/1903.11532)
* DFineNet: Ego-Motion Estimation and Depth Refinement from Sparse, Noisy Depth Input with RGB Guidance
  + [paper](https://arxiv.org/abs/1903.06397)
* Bilateral Cyclic Constraint and Adaptive Regularization for Unsupervised Monocular Depth Prediction
  + [paper](https://arxiv.org/abs/1903.07309)
* Anytime Stereo Image Depth Estimation on Mobile Devices(**)
  + [paper](https://arxiv.org/abs/1810.11408)
* Refine and Distill: Exploiting Cycle-Inconsistency and Knowledge Distillation for Unsupervised Monocular Depth Estimation(CVPR2019)
  + [paper](https://arxiv.org/abs/1903.04202)
* Self-supervised Learning for Single View Depth and Surface Normal Estimation
  + [paper](https://arxiv.org/abs/1903.00112)
* DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image
  + [paper](https://arxiv.org/abs/1812.00488v1)
* A Motion Free Approach to Dense Depth Estimation in Complex Dynamic Scene
  + [paper](https://arxiv.org/abs/1902.03791)
* Self-supervised Learning for Dense Depth Estimation in Monocular Endoscopy
  + [paper](https://arxiv.org/abs/1902.07766?context=cs)
* Region Deformer Networks for Unsupervised Depth Estimation from Unconstrained Monocular Videos
  + [paper](https://arxiv.org/abs/1902.09907)
* Single Image Deblurring and Camera Motion Estimation with Depth Map
  + [paper](https://arxiv.org/abs/1903.00231)
* SweepNet: Wide-baseline Omnidirectional Depth Estimation
  + [paper](https://arxiv.org/abs/1902.10904)
* Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference
  + [paper](https://arxiv.org/abs/1902.10556)
* Multi-layer Depth and Epipolar Feature Transformers for 3D Scene Reconstruction
  + [paper](https://arxiv.org/abs/1902.06729)
* Depth-Map Generation using Pixel Matching in Stereoscopic Pair of Images
  + [paper](https://arxiv.org/abs/1902.03471)
* Unstructured Multi-View Depth Estimation Using Mask-Based Multiplane Representation
  + [paper](https://arxiv.org/abs/1902.02166)
* DFuseNet: Deep Fusion of RGB and Sparse Depth Information for Image Guided Dense Depth Completion
  + [paper](https://arxiv.org/abs/1902.00761)
* Attention-based Context Aggregation Network for Monocular Depth Estimation
  + [paper](https://arxiv.org/abs/1901.10137)
  ![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/Depth%20Prediction%20Without%20the%20Sensors.PNG "Depth Prediction Without the Sensors")
* Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular Videos(已复现)
  + [paper](https://arxiv.org/pdf/1811.06152.pdf)
  + [code](https://github.com/tensorflow/models/tree/master/research/struct2depth)
  + [webpage](https://sites.google.com/view/struct2depth)
  
   
  
   
## 2018
![结构图](https://github.com/lawy623/SVS/raw/master/figs/result.png)
* Single View Stereo Matching
  + [paper](https://arxiv.org/abs/1803.02612)
  + [code](https://github.com/lawy623/SVS)

* Geometry meets semantics for semi-supervised monocular depth estimation
  + [paper](https://arxiv.org/pdf/1810.04093v2.pdf)
  + [code](https://github.com/CVLAB-Unibo/Semantic-Mono-Depth)
* Learning Monocular Depth by Distilling Cross-domain Stereo Networks
  + [paper](https://arxiv.org/pdf/1808.06586v1.pdf)
  + [code](https://github.com/xy-guo/Learning-Monocular-Depth-by-Stereo)
* Revisiting Single Image Depth Estimation: Toward Higher Resolution Maps with Accurate Object Boundaries
  + [paper](https://arxiv.org/pdf/1803.08673v2.pdf)
  + [code](https://github.com/JunjH/Revisiting_Single_Depth_Estimation)
* Deep Ordinal Regression Network for Monocular Depth Estimation
  + [paper](https://arxiv.org/pdf/1806.02446v1.pdf)
  + [code](https://github.com/hufu6371/DORN)
* High Quality Monocular Depth Estimation via Transfer Learning
  + [paper](https://arxiv.org/pdf/1812.11941v2.pdf)
  + [code](https://github.com/ialhashim/DenseDepth)
* Digging Into Self-Supervised Monocular Depth Estimation
  + [paper](https://arxiv.org/pdf/1806.01260v3.pdf)
  + [code](https://github.com/nianticlabs/monodepth2)
* On the Importance of Stereo for Accurate Depth Estimation: An Efficient Semi-Supervised Deep Neural Network Approach
  + [paper](https://arxiv.org/pdf/1803.09719v3.pdf)
  + [code](https://github.com/NVIDIA-AI-IOT/redtail)

<div align=center><img width="200" height="150" src="https://camo.githubusercontent.com/bfeda04ca5dc388a4977c3584ca5b82386bc6f91/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f7669643264657074682f6d656469612f73616d706c655f766964656f5f736d616c6c2e676966"/></div>
<div align=center><img width="400" height="500" src="https://camo.githubusercontent.com/878c3b95ff7066d1294c3b2c72a246d035f85db7/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f7669643264657074682f6d656469612f617070726f6163682e706e67"/></div>

* Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints
  + [paper](https://arxiv.org/pdf/1802.05522.pdf)
  + [code](https://github.com/tensorflow/models/tree/master/research/vid2depth)
  + [Project website](https://sites.google.com/view/vid2depth)
![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/Depth%20VO%20Feat.PNG "Depth-VO-Feat")
* Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction
  + [paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.pdf)
  + [code](https://github.com/Huangying-Zhan/Depth-VO-Feat)
![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/UnDepthflow.PNG "UnDepthflow")
* Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos
  + [paper](https://arxiv.org/pdf/1810.03654.pdf)
  + [code](https://github.com/baidu-research/UnDepthflow)
* DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency
  + [paper](https://arxiv.org/pdf/1809.01649.pdf)
  + [code](https://github.com/vt-vl-lab/DF-Net)
  + [webpage](http://yuliang.vision/DF-Net/)
  
 ![结构图]( https://github.com/yzcjtr/GeoNet/raw/master/misc/overview.jpg)
 * GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose (CVPR 2018)
   + [paper](https://arxiv.org/abs/1803.02276)
   + [code]( https://github.com/yzcjtr/GeoNet)
 * GeoNet:Geometric Neural Network for Joint Depth and Surface Normal Estimation
   + [paper]()
* Driving Scene Perception Network: Real-time Joint Detection, Depth Estimation and Semantic Segmentation
  + [paper]( https://arxiv.org/pdf/1803.03778.pdf)
* Epipolar Geometry based Learning of Multi-view Depth and Ego-Motion from Monocular Sequences
  + [paper](https://arxiv.org/abs/1812.11922)
* SfMLearner++: Learning Monocular Depth & Ego-Motion using Meaningful Geometric Constraints
  + [paper](https://arxiv.org/abs/1812.08370)
* Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion
  + [paper](https://arxiv.org/abs/1812.08156)
* Fast and Accurate Depth Estimation from Sparse Light Fields
  + [paper](https://arxiv.org/abs/1812.06856)
* Geometry meets semantics for semi-supervised monocular depth estimation
  + [paper](https://arxiv.org/abs/1810.04093)
* Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos
  + [paper](https://arxiv.org/abs/1810.03654)
* Learning Depth with Convolutional Spatial Propagation Network
  + [paper](https://arxiv.org/abs/1810.02695)
* SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation
  + [paper](https://arxiv.org/abs/1810.01849)
* CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction
  + [paper](https://arxiv.org/abs/1810.01011)
* Unsupervised Learning of Dense Optical Flow, Depth and Egomotion from Sparse Event Data
  + [paper](https://arxiv.org/abs/1809.08625)
* GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks
  + [paper](https://arxiv.org/abs/1809.05786)
* Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations
  + [paper](https://arxiv.org/abs/1809.04766)
* DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency
  + [paper](https://arxiv.org/abs/1809.01649)
* Sparse-to-Continuous: Enhancing Monocular Depth Estimation using Occupancy Maps
  + [paper](https://arxiv.org/abs/1809.09061)
* Deep Depth from Defocus: how can defocus blur improve 3D estimation using dense neural networks?
  + [paper](https://arxiv.org/abs/1809.01567)
* A Deeper Insight into the UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation
  + [paper](https://arxiv.org/abs/1809.00969)
* Detail Preserving Depth Estimation from a Single Image Using Attention Guided Networks
  + [paper](https://arxiv.org/abs/1809.00646)
* Rethinking Monocular Depth Estimation with Adversarial Training
  + [paper](https://arxiv.org/abs/1808.07528)
* Deeply Supervised Depth Map Super-Resolution as Novel View Synthesis
  + [paper](https://arxiv.org/abs/1808.08688)
* LEGO: Learning Edge with Geometry all at Once by Watching Videos
  + [paper](https://arxiv.org/abs/1803.05648)
  + [code](https://github.com/zhenheny/LEGO)
* Learning monocular visual odometry with dense 3D mapping from dense 3D flow
  + [paper](https://arxiv.org/pdf/1803.02286.pdf)
  
![结构图](https://github.com/MightyChaos/MightyChaos.github.io/raw/master/projects/cvpr18_chaoyang/demo.gif)
![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/LKVOLearner.PNG)

* Learning Depth from Monocular Videos using Direct Methods
  + [paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Learning_Depth_From_CVPR_2018_paper.pdf)
  + [code](https://github.com/MightyChaos/LKVOLearner)
  
## 2017
* Unsupervised Monocular Depth Estimation with Left-Right Consistency  
  + [paper](https://arxiv.org/pdf/1609.03677v3.pdf)  
  + [code tensorflow](https://github.com/mrharicot/monodepth)  
  + [code pytorch](https://github.com/alwynmathew/monodepth-pytorch)
* A Two-Streamed Network for Estimating Fine-Scaled Depth Maps from Single RGB Images
  + [paper](https://arxiv.org/abs/1607.00730)
* Single-Image Depth Perception in the Wild
  + [paper](https://arxiv.org/abs/1604.03901)
*  Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints
   + [paper](https://arxiv.org/pdf/1802.05522v1.pdf)  
   
 ![结构图]( https://github.com/sxfduter/monocular-depth-estimation/blob/master/SfMLearner1.PNG "SfMLearner1") 
* Unsupervised Learning of Depth and Ego-Motion from Video(已复现)
  + [paper](https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/cvpr17_sfm_final.pdf)
  + [code](https://github.com/tinghuiz/SfMLearner)
  + [project webpage](https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/)
## 2016



* Fast Robust Monocular Depth Estimation for Obstacle Detection with Fully Convolutional Networks
  + [paper](https://arxiv.org/pdf/1607.06349v1.pdf)
* Parse Geometry from a Line: Monocular Depth Estimation with Partial Laser Observation
  + [paper](https://arxiv.org/pdf/1611.02174v1.pdf)
* Joint Semantic Segmentation and Depth Estimation with Deep Convolutional Networks
  + [paper](https://arxiv.org/abs/1604.07480v1)
* Deeper Depth Prediction with Fully Convolutional Residual Networks
  + [paper](https://arxiv.org/abs/1606.00373)
* Deep3D Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks
  + [paper](https://arxiv.org/abs/1604.03650v1)
## 2015
* Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture
  + [paper](https://arxiv.org/abs/1411.4734)
## 2014
* Deep Convolutional Neural Fields for Depth Estimation from a Single Image 
  + [paper](https://arxiv.org/abs/1411.6387)
* Depth Map Prediction from a Single Image using a Multi-Scale Deep Network
  + [paper](https://arxiv.org/abs/1406.2283v1)
  + [code for pytorch](https://github.com/DhruvJawalkar/Depth-Map-Prediction-from-a-Single-Image-using-a-Multi-Scale-Deep-Network)
  


  


## review
* Monocular Depth Estimation: A Survey
  + [paper](https://arxiv.org/abs/1901.09402)
* A Survey on Deep Learning Architectures for Image-based Depth Reconstruction
  + [paper](https://arxiv.org/abs/1906.06113)


## Network Architecture
* ResNet: Deep Residual Learning for Image Recognition
  + [paper](https://arxiv.org/abs/1512.03385)
* VGG: Very Deep Convolutional Networks for Large-scale Image Recognition
  + [paper](https://arxiv.org/abs/1409.1556)
* Squeeze-and-Excitation Networks
  + [paper]( https://arxiv.org/abs/1709.01507v1)
* CBAM: Convolutional Block Attention Module
  + [paper]( http://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)
* GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond
  + [paper](https://arxiv.org/abs/1904.11492?context=cs.LG)
* ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices
  + [paper]( https://arxiv.org/abs/1707.01083)
* ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design
  + [paper](https://arxiv.org/abs/1807.11164)
* MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
  + [paper](https://arxiv.org/abs/1704.04861)
* MobileNetV2: Inverted Residuals and Linear Bottlenecks
  + [paper](https://arxiv.org/abs/1801.04381)
  
  
 
  
 

