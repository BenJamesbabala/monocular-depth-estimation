# monocular-depth-estimation

## 2019
![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/multitask%20refinenet.PNG "multitask%20refinenet")
* Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations
  + [paper](https://arxiv.org/pdf/1809.04766.pdf)
  + [code](https://github.com/drsleep/multi-task-refinenet)
![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/Competitive%20Collaboration.PNG "Competitive%20Collaboration")
* Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera
Motion, Optical Flow and Motion Segmentation
  + [paper](https://arxiv.org/pdf/1805.09806.pdf)
  + [code](https://github.com/anuragranj/cc)
  
* Privacy Protection in Street-View Panoramas using Depth and Multi-View Imagery
  + [paper](https://arxiv.org/abs/1903.11532)
* DFineNet: Ego-Motion Estimation and Depth Refinement from Sparse, Noisy Depth Input with RGB Guidance
  + [paper](https://arxiv.org/abs/1903.06397)
* Bilateral Cyclic Constraint and Adaptive Regularization for Unsupervised Monocular Depth Prediction
  + [paper](https://arxiv.org/abs/1903.07309)
* Anytime Stereo Image Depth Estimation on Mobile Devices(**)
  + [paper](https://arxiv.org/abs/1810.11408)
* Refine and Distill: Exploiting Cycle-Inconsistency and Knowledge Distillation for Unsupervised Monocular Depth Estimation(CVPR2019)
  + [paper](https://arxiv.org/abs/1903.04202)
* Self-supervised Learning for Single View Depth and Surface Normal Estimation
  + [paper](https://arxiv.org/abs/1903.00112)
* DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image
  + [paper](https://arxiv.org/abs/1812.00488v1)
* A Motion Free Approach to Dense Depth Estimation in Complex Dynamic Scene
  + [paper](https://arxiv.org/abs/1902.03791)
* Self-supervised Learning for Dense Depth Estimation in Monocular Endoscopy
  + [paper](https://arxiv.org/abs/1902.07766?context=cs)
* Region Deformer Networks for Unsupervised Depth Estimation from Unconstrained Monocular Videos
  + [paper](https://arxiv.org/abs/1902.09907)
* Single Image Deblurring and Camera Motion Estimation with Depth Map
  + [paper](https://arxiv.org/abs/1903.00231)
* SweepNet: Wide-baseline Omnidirectional Depth Estimation
  + [paper](https://arxiv.org/abs/1902.10904)
* Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference(重点)
  + [paper](https://arxiv.org/abs/1902.10556)
* Multi-layer Depth and Epipolar Feature Transformers for 3D Scene Reconstruction
  + [paper](https://arxiv.org/abs/1902.06729)
* Depth-Map Generation using Pixel Matching in Stereoscopic Pair of Images
  + [paper](https://arxiv.org/abs/1902.03471)
* Unstructured Multi-View Depth Estimation Using Mask-Based Multiplane Representation
  + [paper](https://arxiv.org/abs/1902.02166)
* DFuseNet: Deep Fusion of RGB and Sparse Depth Information for Image Guided Dense Depth Completion
  + [paper](https://arxiv.org/abs/1902.00761)
* Attention-based Context Aggregation Network for Monocular Depth Estimation
  + [paper](https://arxiv.org/abs/1901.10137)
  ![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/Depth%20Prediction%20Without%20the%20Sensors.PNG "Depth Prediction Without the Sensors")
* Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular Videos(已复现)
  + [paper](https://arxiv.org/pdf/1811.06152.pdf)
  + [code](https://github.com/tensorflow/models/tree/master/research/struct2depth)
  + [webpage](https://sites.google.com/view/struct2depth)
  
   
  
   
## 2018



![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/Depth%20VO%20Feat.PNG "Depth-VO-Feat")
* Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction
  + [paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhan_Unsupervised_Learning_of_CVPR_2018_paper.pdf)
  + [code](https://github.com/Huangying-Zhan/Depth-VO-Feat)
![结构图](https://github.com/sxfduter/monocular-depth-estimation/blob/master/UnDepthflow.PNG "UnDepthflow")
* Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos
  + [paper](https://arxiv.org/pdf/1810.03654.pdf)
  + [code](https://github.com/baidu-research/UnDepthflow)
* DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency
  + [paper](https://arxiv.org/pdf/1809.01649.pdf)
  + [code](https://github.com/vt-vl-lab/DF-Net)
  + [webpage](http://yuliang.vision/DF-Net/)
  
 
* Driving Scene Perception Network: Real-time Joint Detection, Depth Estimation and Semantic Segmentation
  + [paper]( https://arxiv.org/pdf/1803.03778.pdf)
* Epipolar Geometry based Learning of Multi-view Depth and Ego-Motion from Monocular Sequences
  + [paper](https://arxiv.org/abs/1812.11922)
* SfMLearner++: Learning Monocular Depth & Ego-Motion using Meaningful Geometric Constraints
  + [paper](https://arxiv.org/abs/1812.08370)
* Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion
  + [paper](https://arxiv.org/abs/1812.08156)
* Fast and Accurate Depth Estimation from Sparse Light Fields
  + [paper](https://arxiv.org/abs/1812.06856)
* Geometry meets semantics for semi-supervised monocular depth estimation
  + [paper](https://arxiv.org/abs/1810.04093)
* Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos
  + [paper](https://arxiv.org/abs/1810.03654)
* Learning Depth with Convolutional Spatial Propagation Network
  + [paper](https://arxiv.org/abs/1810.02695)
* SuperDepth: Self-Supervised, Super-Resolved Monocular Depth Estimation
  + [paper](https://arxiv.org/abs/1810.01849)
* CNN-SVO: Improving the Mapping in Semi-Direct Visual Odometry Using Single-Image Depth Prediction
  + [paper](https://arxiv.org/abs/1810.01011)
* Unsupervised Learning of Dense Optical Flow, Depth and Egomotion from Sparse Event Data
  + [paper](https://arxiv.org/abs/1809.08625)
* GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks
  + [paper](https://arxiv.org/abs/1809.05786)
* Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations
  + [paper](https://arxiv.org/abs/1809.04766)
* DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency
  + [paper](https://arxiv.org/abs/1809.01649)
* Sparse-to-Continuous: Enhancing Monocular Depth Estimation using Occupancy Maps
  + [paper](https://arxiv.org/abs/1809.09061)
* Deep Depth from Defocus: how can defocus blur improve 3D estimation using dense neural networks?
  + [paper](https://arxiv.org/abs/1809.01567)
* A Deeper Insight into the UnDEMoN: Unsupervised Deep Network for Depth and Ego-Motion Estimation
  + [paper](https://arxiv.org/abs/1809.00969)
* Detail Preserving Depth Estimation from a Single Image Using Attention Guided Networks
  + [paper](https://arxiv.org/abs/1809.00646)
* Rethinking Monocular Depth Estimation with Adversarial Training
  + [paper](https://arxiv.org/abs/1808.07528)
* Deeply Supervised Depth Map Super-Resolution as Novel View Synthesis
  + [paper](https://arxiv.org/abs/1808.08688)
* LEGO: Learning Edge with Geometry all at Once by Watching Videos
  + [paper](https://arxiv.org/abs/1803.05648)
  + [code](https://github.com/zhenheny/LEGO)
* Learning monocular visual odometry with dense 3D mapping from dense 3D flow
  + [paper](https://arxiv.org/pdf/1803.02286.pdf)
  
  
  
  
  
  
  
  

  
  
  

 
  
  
  
  
  
  
  
  
  
  
  
  
## 2017
* A Two-Streamed Network for Estimating Fine-Scaled Depth Maps from Single RGB Images
  + [paper](https://arxiv.org/abs/1607.00730)
* Single-Image Depth Perception in the Wild
  + [paper](https://arxiv.org/abs/1604.03901)
  
 ![结构图]( https://github.com/sxfduter/monocular-depth-estimation/blob/master/SfMLearner1.PNG "SfMLearner1") 
* Unsupervised Learning of Depth and Ego-Motion from Video(已复现)
  + [paper](https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/cvpr17_sfm_final.pdf)
  + [code](https://github.com/tinghuiz/SfMLearner)
  + [project webpage](https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/)
## 2016
* Joint Semantic Segmentation and Depth Estimation with Deep Convolutional Networks
  + [paper](https://arxiv.org/abs/1604.07480v1)
* Deeper Depth Prediction with Fully Convolutional Residual Networks
   + [paper](https://arxiv.org/abs/1606.00373)
* Deep3D Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks
  + [paper](https://arxiv.org/abs/1604.03650v1)
## 2015
* Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture
  + [paper](https://arxiv.org/abs/1411.4734)
## 2014
* Deep Convolutional Neural Fields for Depth Estimation from a Single Image 
  + [paper](https://arxiv.org/abs/1411.6387)
* Depth Map Prediction from a Single Image using a Multi-Scale Deep Network
  + [paper](https://arxiv.org/abs/1406.2283v1)
  + [code for pytorch](https://github.com/DhruvJawalkar/Depth-Map-Prediction-from-a-Single-Image-using-a-Multi-Scale-Deep-Network)
  


  


## review
* Monocular Depth Estimation: A Survey
  + [paper](https://arxiv.org/abs/1901.09402)

## Network Architecture
* ResNet: Deep Residual Learning for Image Recognition
  + [paper](https://arxiv.org/abs/1512.03385)
* VGG: Very Deep Convolutional Networks for Large-scale Image Recognition
  + [paper](https://arxiv.org/abs/1409.1556)
* Squeeze-and-Excitation Networks
  + [paper]( https://arxiv.org/abs/1709.01507v1)
* ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices
  + [paper]( https://arxiv.org/abs/1707.01083)
* ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design
  + [paper](https://arxiv.org/abs/1807.11164)
* MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
  + [paper](https://arxiv.org/abs/1704.04861)
* MobileNetV2: Inverted Residuals and Linear Bottlenecks
  + [paper](https://arxiv.org/abs/1801.04381)
  
  
 
  
 

